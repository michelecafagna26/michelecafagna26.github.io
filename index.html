<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Michele Cafagna</title> <meta name="author" content="Michele Cafagna"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon_io_2/favicon.ico?f595e56dd1466e26a24c2bb269824344"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://michelecafagna26.github.io//"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?3dd82e91913a2c1265c0f80e41ff39e2"></script> <script src="/assets/js/dark_mode.js?6458e63976eae16c0cbe86b97023895a"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Short CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Michele</span> Cafagna </h1> <p class="desc">NLP Data Scientist @<a href="https://www.um.edu.mt/" rel="external nofollow noopener" target="_blank">Okra.ai</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/avatar-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/avatar-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/avatar-1400.webp"></source> <img src="/assets/img/avatar.png?459a1404788f361b5f508dd3ea75aaad" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="avatar.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> <p>Leiden, The Netherlands</p> </div> </div> <div class="clearfix"> <p>Iâ€™m Michele [miËˆkÉ›Ële], NLP R&amp;D Scientist at <a href="https://okra.ai/" rel="external nofollow noopener" target="_blank">Okra.ai</a>, working in Medical NLP. My primary interests lie at the intersection of <strong>Computer Vision, Natural Language, Cognitive Sciences</strong>, and <strong>XAI</strong>.</p> <p>Iâ€™ve received my Phd from the <a href="https://www.um.edu.mt/linguistics/" rel="external nofollow noopener" target="_blank">Institute of Linguistics &amp; Language Technology</a> at the University of Malta, ğŸ‡²ğŸ‡¹, supervised by Prof. <a href="https://albertgatt.github.io/" rel="external nofollow noopener" target="_blank">Albert Gatt</a> and co-supervised by Prof. <a href="https://www.uu.nl/staff/CJvanDeemter" rel="external nofollow noopener" target="_blank">Kees van Deemter</a>. Here I was a Marie Curie PhD Fellow and Early Stage Researcher in the <a href="https://nl4xai.eu/" rel="external nofollow noopener" target="_blank">NL4XAI Project</a> During my Phd, I worked as a Visiting Researcher at the <a href="https://www.uu.nl/en" rel="external nofollow noopener" target="_blank">University of Utrecht</a>, ğŸ‡³ğŸ‡± focusing on Multimodal Grounding and interning at the ğŸŠOrange Innovations Labs, Lannion, ğŸ‡«ğŸ‡·.</p> <p>Previously, I was Machine Learning Research Scientist at Aptus.AI, a RegTech startup based in Pisa, ğŸ‡®ğŸ‡¹. I earned my Masterâ€™s in Computer Science and AI, at the <a href="https://www.unipi.it/" rel="external nofollow noopener" target="_blank">University of Pisa</a> ğŸ‡®ğŸ‡¹, with a thesis project in NLG carried out as a Visiting Researcher at the <a href="https://www.rug.nl/research/clcg/?lang=en" rel="external nofollow noopener" target="_blank">Center for Language and Cognition of the University of Groeningen (CLCG)</a>, ğŸ‡³ğŸ‡± . Iâ€™ve also collaborated with the <a href="http://www.italianlp.it/" rel="external nofollow noopener" target="_blank">ItaliaNLP Lab</a> at the Institute of Linguistics of the National Research Center (ILC-CNR) based in Pisa, ğŸ‡®ğŸ‡¹.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">May 15, 2024</th> <td> My Phd Thesis is online ğŸ‰ : <a href="https://www.um.edu.mt/library/oar/bitstream/123456789/127773/1/2401LLTLIN600005073698_1.PDF" rel="external nofollow noopener" target="_blank">Visually Grounded Language Generation: Data, Models and Explanations beyond Descriptive Captions</a>. </td> </tr> <tr> <th scope="row">Mar 18, 2024</th> <td> Joined <a href="https://okra.ai/" rel="external nofollow noopener" target="_blank"> Okra.ai</a> as NLP Data Scentist,ğŸ‡³ğŸ‡± ğŸ‰ </td> </tr> <tr> <th scope="row">Jan 16, 2024</th> <td> Our paper<a href="https://openreview.net/forum?id=liuqDwmbQJ" rel="external nofollow noopener" target="_blank"> â€œViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Modelsâ€</a> , accepted @ICLR 2024 , Vienna,ğŸ‡¦ğŸ‡¹ ğŸ‰ </td> </tr> <tr> <th scope="row">Oct 10, 2023</th> <td> Reviewer for<a href="https://lrec-coling-2024.org/" rel="external nofollow noopener" target="_blank"> COLING-LREC2024</a>, Torino, ğŸ‡®ğŸ‡¹ </td> </tr> <tr> <th scope="row">Sep 11, 2023</th> <td> Presented the poster<a href="https://sigdialinlg2023.github.io/static/posters/INLG2023/13.pdf" rel="external nofollow noopener" target="_blank">â€œHL Dataset: Visually-grounded Description of Scenes, Actions and Rationalesâ€ </a><a href="https://inlg2023.github.io/" rel="external nofollow noopener" target="_blank"> @INLG 2023 </a> , Prague, ğŸ‡¨ğŸ‡¿ </td> </tr> <tr> <th scope="row">Sep 4, 2023</th> <td> Our paper <a href="https://www.frontiersin.org/articles/10.3389/frai.2023.1220476/abstract" rel="external nofollow noopener" target="_blank"> â€œInterpreting Vision and Language Generative Models with Semantic Visual Priorsâ€</a> published in <a href="https://www.frontiersin.org/research-topics/48440/explainable-ai-in-natural-language-processing" rel="external nofollow noopener" target="_blank"> @Frontiers in AI Journal </a> </td> </tr> <tr> <th scope="row">Aug 6, 2023</th> <td> Reviewer for<a href="https://ecai2023.eu/" rel="external nofollow noopener" target="_blank"> 26th European Conference on Artificial Intelligence ECAI 2023</a>, KrakÃ³w, ğŸ‡µğŸ‡± </td> </tr> <tr> <th scope="row">Jul 13, 2023</th> <td> Our paper <a href="https://arxiv.org/abs/2302.12189?context=cs.CL" rel="external nofollow noopener" target="_blank">â€œHL Dataset: Visually-grounded Description of Scenes, Actions and Rationalesâ€ </a>, accepted <a href="https://inlg2023.github.io/" rel="external nofollow noopener" target="_blank"> @INLG 2023 </a> , Prague, ğŸ‡¨ğŸ‡¿ ğŸ‰ </td> </tr> <tr> <th scope="row">Jun 18, 2023</th> <td> Reviewer for<a href="https://synalp.gitlabpages.inria.fr/mmnlg2023/" rel="external nofollow noopener" target="_blank"> MMNLG2023 co-located with INLG 2023</a>, Prague, ğŸ‡¨ğŸ‡¿ </td> </tr> <tr> <th scope="row">Jun 14, 2023</th> <td> Reviewer for<a href="https://2023.emnlp.org/" rel="external nofollow noopener" target="_blank"> EMNLP2023</a>, Singapore, ğŸ‡¸ğŸ‡¬ </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="https://images.deepai.org/publication-preview/interpreting-vision-and-language-generative-models-with-semantic-visual-priors-page-9-medium.jpg"></div> <div id="10.3389/frai.2023.1220476" class="col-sm-8"> <div class="title">Interpreting vision and language generative models with semantic visual priors</div> <div class="author"> <em>Michele Cafagna</em>,Â Lina M. Rojas-Barahona,Â Kees Deemter,Â andÂ Albert Gatt</div> <div class="periodical"> <em>Frontiers in Artificial Intelligence</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.frontiersin.org/articles/10.3389/frai.2023.1220476/full" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/michelecafagna26/vl-shap" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>When applied to Image-to-text models, explainability methods have two challenges. First, they often provide token-by-token explanations namely, they compute a visual explanation for each token of the generated sequence. This makes explanations expensive to compute and unable to comprehensively explain the modelâ€™s output. Second, for models with visual inputs, explainability methods such as SHAP typically consider superpixels as features. Since superpixels do not correspond to semantically meaningful regions of an image, this makes explanations harder to interpret. We develop a framework based on SHAP, that allows for generating comprehensive, meaningful explanations leveraging the meaning representation of the output sequence as a whole. Moreover, by exploiting semantic priors in the visual backbone, we extract an arbitrary number of features that allows the efficient computation of Shapley values on large-scale models, generating at the same time highly meaningful visual explanations. We demonstrate that our method generates semantically more expressive explanations than traditional methods at a lower compute cost and that it can be generalized to a large family of vision-language models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="https://images.deepai.org/publication-preview/valse-a-task-independent-benchmark-for-vision-and-language-models-centered-on-linguistic-phenomena-page-20-medium.jpg"></div> <div id="parcalabescu-etal-2022-valse" class="col-sm-8"> <div class="title">VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena</div> <div class="author"> Letitia Parcalabescu,Â <em>Michele Cafagna</em>,Â Lilitta Muradjan,Â Anette Frank,Â Iacer Calixto,Â andÂ Albert Gatt</div> <div class="periodical"> <em>In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.acl-long.567" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Heidelberg-NLP/VALSE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&amp;L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V&amp;L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V&amp;L models from a linguistic perspective, complementing the canonical task-centred V&amp;L evaluations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="https://images.deepai.org/publication-preview/hl-dataset-grounding-high-level-linguistic-concepts-in-vision-page-3-medium.jpg"></div> <div id="cafagna-etal-2023-hl" class="col-sm-8"> <div class="title">HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales</div> <div class="author"> <em>Michele Cafagna</em>,Â Kees Deemter,Â andÂ Albert Gatt</div> <div class="periodical"> <em>In Proceedings of the 16th International Natural Language Generation Conference</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2023.inlg-main.21" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/michelecafagna26/HL-dataset" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sigdialinlg2023.github.io/static/posters/INLG2023/13.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="abstract hidden"> <p>Current captioning datasets focus on object-centric captions, describing the visible objects in the image, often ending up stating the obvious (for humans), e.g. â€œpeople eating food in a parkâ€. Although these datasets are useful to evaluate the ability of Vision &amp; Language models to recognize and describe visual content, they do not support controlled experiments involving model testing or fine-tuning, with more high-level captions, which humans find easy and natural to produce. For example, people often describe images based on the type of scene they depict (â€œpeople at a holiday resortâ€) and the actions they perform (â€œpeople having a picnicâ€). Such concepts are based on personal experience and contribute to forming common sense assumptions. We present the High-Level Dataset, a dataset extending 14997 images from the COCO dataset, aligned with a new set of 134,973 human-annotated (high-level) captions collected along three axes: scenes, actions and rationales. We further extend this dataset with confidence scores collected from an independent set of readers, as well as a set of narrative captions generated synthetically, by combining each of the three axes. We describe this dataset and analyse it extensively. We also present baseline results for the High-Level Captioning task.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="https://images.deepai.org/publication-preview/understanding-cross-modal-interactions-in-v-l-models-that-generate-scene-descriptions-page-1-medium.jpg"></div> <div id="cafagna-etal-2022-understanding" class="col-sm-8"> <div class="title">Understanding Cross-modal Interactions in V&amp;L Models that Generate Scene Descriptions</div> <div class="author"> <em>Michele Cafagna</em>,Â Kees van Deemter,Â andÂ Albert Gatt</div> <div class="periodical"> <em>In Proceedings of the Workshop on Unimodal and Multimodal Induction of Linguistic Structures (UM-IoS)</em>, Dec 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.umios-1.6" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Image captioning models tend to describe images in an object-centric way, emphasising visible objects. But image descriptions can also abstract away from objects and describe the type of scene depicted. In this paper, we explore the potential of a state of the art Vision and Language model, VinVL, to caption images at the scene level using (1) a novel dataset which pairs images with both object-centric and scene descriptions. Through (2) an in-depth analysis of the effect of the fine-tuning, we show (3) that a small amount of curated data suffices to generate scene descriptions without losing the capability to identify object-level concepts in the scene; the model acquires a more holistic view of the image compared to when object-centric descriptions are generated. We discuss the parallels between these results and insights from computational and cognitive science research on scene perception.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="https://images.deepai.org/publication-preview/what-vision-language-models-see-when-they-see-scenes-page-1-medium.jpg"></div> <div id="cafagna2021vision" class="col-sm-8"> <div class="title">What Vision-Language Models â€™Seeâ€™ when they See Scenes</div> <div class="author"> <em>Michele Cafagna</em>,Â Kees Deemter,Â andÂ Albert Gatt</div> <div class="periodical"> <em>arXiv preprint arXiv:2109.07301</em>, Dec 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2109.07301" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/abs/2109.07301" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/michelecafagna26/vl-ablation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Images can be described in terms of the objects they contain, or in terms of the types of scene or place that they instantiate. In this paper we address to what extent pretrained Vision and Language models can learn to align descriptions of both types with images. We compare 3 state-of-the-art models, VisualBERT, LXMERT and CLIP. We find that (i) V&amp;L models are susceptible to stylistic biases acquired during pretraining; (ii) only CLIP performs consistently well on both object- and scene-level descriptions. A follow-up ablation study shows that CLIP uses object-level information in the visual modality to align with scene-level textual descriptions.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6D%69%63%68%65%6C%65.%63%61%66%61%67%6E%61{%61%74}%75%6D.%65%64%75.%6D%74" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-1297-3729" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=GOXgBVAAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/2127130978" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/michelecafagna26" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/michele-cafagna" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/cafagna_m" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://dblp.org/pid/252/0944.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> <a href="https://huggingface.co/michelecafagna26" title="HuggingFace" rel="external nofollow noopener" target="_blank"><i class="fa-regular fa-face-grin-beam"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2025 Michele Cafagna. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>